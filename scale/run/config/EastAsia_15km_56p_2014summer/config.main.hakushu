#!/bin/bash
#===============================================================================
#
#  Main settings for SCALE-LETKF scripts
#
#===============================================================================

DIR="$(cd "$(pwd)/.." && pwd)"   # Root directory of the SCALE-LETKF

OUTDIR="/data5/gylien/exp/scale-letkf_test/EastAsia_15km_48p" # Directory for SCALE-LETKF output

LOGDIR="$DIR/log"

#===============================================================================
# Location of model/data files

MODELDIR="/data6/gylien/scale-letkf/scale/scale-rm/test/case_real/check_japan/run"
DATADIR="/data6/gylien/scale-letkf/database"

DATA_TOPO="/data6/gylien/scale-letkf/database/topo/EastAsia_15km_48p"
DATA_LANDUSE="/data6/gylien/scale-letkf/database/landuse/EastAsia_15km_48p"
DATA_BDY_SCALE=
DATA_BDY_WRF="/data6/gylien/model/ncepfnl_large/wrfout"
DATA_BDY_NICAM=

OBS="/data6/gylien/obs/prepbufr_obs_eastasia" # directory of observation data in LETKF obs format
OBSNCEP="/data6/gylien/obs/prepbufr"  # directory of observation data in NCEP BUFR format

#===============================================================================
# model/data file options

TOPO_FORMAT='GTOPO30'   # 'prep': Use prepared topo files in $DATA_TOPO
                        # 'GTOPO30' (requires compatible 'config.nml.scale_pp')
                        # 'DEM50M'  (requires compatible 'config.nml.scale_pp')

LANDUSE_FORMAT='GLCCv2' # 'prep': Use prepared landuse files in $DATA_LANDUSE
                        # 'GLCCv2' (requires compatible 'config.nml.scale_pp')
                        # 'LU100M' (requires compatible 'config.nml.scale_pp')
LANDUSE_UPDATE=0        # 0: Time-invariant landuse files
                        # 1: Time-variant landuse files

BDY_FORMAT=2            # 0: SCALE with exactly same domain settings (do not need additional preprocessing)
                        # 1: SCALE (requires compatible 'config.nml.scale_init')
                        # 2: WRF   (requires compatible 'config.nml.scale_init')
                        # 3: NICAM (requires compatible 'config.nml.scale_init')
BDY_ENS=1               # 0: Fixed boundary files for all memebers
                        # 1: Ensemble boundary files

BDYINT=21600

OBSNUM=1
OBSNAME[1]=obs                                    
OBSNAME[2]=radar

#===============================================================================
# Cycling settings

WINDOW_S=10800     # SCALE forecast time when assimilation window starts (second)
WINDOW_E=32400     # SCALE forecast time when assimilation window ends (second)
LCYCLE=21600       # Length of a GFS-LETKF cycle (second)
LTIMESLOT=3600     # Timeslot interval for 4D-LETKF (second)

#===============================================================================
# Parallelization settings

MEMBER=20          # Ensemble size

NNODES=1           # Number of nodes
PPN=48             # Number of processes per node

THREADS=1          # Number of threads per process

SCALE_NP=48        # Number of processes to run SCALE

BGJOB_INT='0.1s'   # Interval of multiple background job submissions

#===============================================================================
# Temporary directories to store runtime files

MACHINE_TYPE=1              # Machine type
                            #  1: Linux cluster with PBS
                            # 10: K-computer
                            # 11: K-computer (micro jobs)
                            # 12: K-computer (interact jobs)

TMPDAT_MODE=3               # Disk type used for the 'dat' temporary directory (input data)
TMPRUN_MODE=3               # Disk type used for the 'run' temporary directory (runtime files)
TMPOUT_MODE=$TMPRUN_MODE    # Disk type used for the 'out' temporary directory (output)
                            #  1: share (link to TMP)
                            #  2: share (staging to TMP)
                            #  3: local (staging to TMPL)

ONLINE_STGOUT=1             # Stage out right after each cycle (do not wait until the end of the job)?
                            #  0: No
                            #  1: Yes

SYSNAME="$(basename $OUTDIR)"                # A unique name in the machine
TMPSUBDIR="scale-letkf_$(whoami)_${SYSNAME}" # (used to identify multiple runs in the same time)

TMP="$DIR/tmp/$TMPSUBDIR"   # Temporary directory shared among all nodes
TMPS="/dev/shm/$TMPSUBDIR"  # Temporary directory only on the server node
TMPL="/dev/shm/$TMPSUBDIR"  # Local temporary directory on computing nodes

#===============================================================================
# Environmental settings

MPIBIN=$(dirname $(which mpirun))
MPIRUN="$MPIBIN/mpirun"

SCP='cp -L'
SCP_HOSTPREFIX=''
#SCP="scp -q"
#SCP_HOSTPREFIX="XXXX:"

PYTHON="python3"

BUFRBIN="/data/opt/bufrlib/10.1.0_intel/bin"

#===============================================================================
# Machine-independent source file

. config.rc

#===============================================================================
